<!DOCTYPE html>
<html lang="en-us" dir="ltr">
<head>
  <meta charset="utf-8">
<meta name="viewport" content="width=device-width">
<title>What is a scientific (batch) workflow? | Living Systems</title>

      <link rel="stylesheet" href="/css/main.min.0dd62280a0c008a6369ab81bd9d63c41bf7e4b5e9377c4a60df43868258f1db5.css" integrity="sha256-DdYigKDACKY2mrgb2dY8Qb9&#43;S16Td8SmDfQ4aCWPHbU=" crossorigin="anonymous">


      <script src="/js/main.23cd0c7d837263b9eaeb96ee2d9ccfa2969daa3fa00fa1c1fe8701a9b87251a1.js" integrity="sha256-I80MfYNyY7nq65buLZzPopadqj&#43;gD6HB/ocBqbhyUaE=" crossorigin="anonymous"></script>



</head>
<body>
  <header>
    <h1>Living Systems</h1>

  <nav>
    <ul>
    <li>
      <a href="/">Home</a>
    </li>
    <li>
      <a aria-current="true" class="ancestor" href="/posts/">Posts</a>
    </li>
    <li>
      <a href="/tags/">Tags</a>
    </li>
    </ul>
  </nav>


  </header>
  <main>
    
  <h1>What is a scientific (batch) workflow?</h1>

  
  
  <time datetime="2017-12-07T00:57:00&#43;01:00">December 7, 2017</time>

  <h2 id="dependency-graph-in-luigi---a-dag-representing-tasks-not-processes-or-workflow-stepsdependencygraphnew_without_shadow-1pngdependencygraphnew_without_shadow-1png"><a href="dependencygraphnew_without_shadow-1.png"><p class="image">
    <img src="dependencygraphnew_without_shadow-1.png" alt="Dependency graph in Luigi - A DAG representing tasks (not processes or workflow steps)"  />
</p>
</a>
</h2>
<h2 id="workflows-and-dags---confusion-about-the-concepts">Workflows and DAGs - Confusion about the concepts</h2>
<p><a href="https://twitter.com/joergenbr" target="_blank" rel="noopener">Jörgen Brandt</a>
 <a href="https://twitter.com/joergenbr/status/907626987333746688" target="_blank" rel="noopener">tweeted a
comment</a>
 that
got me thinking again on something I&rsquo;ve pondered a lot lately:</p>
<blockquote>
<p>&ldquo;A workflow is a DAG.&rdquo; is really a weak definition. That&rsquo;s like
saying &ldquo;A love letter is a sequence of characters.&rdquo; representation ≠
meaning</p>
<p>&ndash; <a href="https://twitter.com/joergenbr/status/907626987333746688" target="_blank" rel="noopener">@joergenbr</a>
</p>
</blockquote>
<p>Jörgen makes a good point. A <a href="https://en.wikipedia.org/wiki/Directed_acyclic_graph" target="_blank" rel="noopener">Directed Acyclic Graph
(DAG)</a>
 does not by
any means capture the full semantic content included in a computational
workflow. I think <a href="http://www.worldscientific.com/doi/pdf/10.1142/9789814508728_0001" target="_blank" rel="noopener">Werner Gitt&rsquo;s <em>universal information</em>
model</a>

is highly relevant here, suggesting that information comes in at least
five abstraction layers: statistics (signals, number of symbols), syntax
(set of symbols, grammar), semantics (meaning), pragmatics (action),
apobetics (purpose, result). A DAG seems to cover the syntax and
semantics layers, leaving out three layers out of five.</p>
<p>Anyways, this also points to another fact: A lot of workflow users have
a less than solid understanding of what workflows actually are. This
leads to further confusion about how different types of workflow tools
actually differ. As a small example, in many workflows-as-DAG
discussions, there is often not even a common understanding of what the
nodes and edges of the DAGs should represent. Are nodes processing
components? tasks? task invocations? &hellip; or data inputs and outputs?
Are the edges data? &hellip; or just dependencies?</p>
<p>It seems the concept of a scientific workflow might be just slightly
more complicated than many realize. The ones who seem to truly
understand the differences are the ones who tried building a workflow
tool themselves (which is one good reason to not always refrain from
trying to build your own tool, despite all the &ldquo;reinventing the wheel&rdquo;
complaints :)).</p>
<p>Anyways, all this confusion is why I&rsquo;m writing this post, hoping to
clarify a little bit what scientific batch workflow systems are at their
core, as far as my experience goes.</p>
<p>There are many ways to describe workflows, but specifically, I&rsquo;m trying
to define <em>the smallest set of information that is always encoded in
workflow definitions, across the most popular shell-based scientific
batch workflows used in bioinformatics today</em>.</p>
<h2 id="some-context">Some context</h2>
<p>Firstly, just some assumptions. In this context, what I mean with a
scientific workflow, or <a href="https://en.wikipedia.org/wiki/Scientific_workflow_system" target="_blank" rel="noopener">scientific workflow system or
tool</a>
 is a set
of shell commands producing outputs, where the commands are depending on
each other in such a way that one command can only start after some
other command has finished. This is how a lot of bioinformatics in
high-throughput disciplines such as Next-Generation Sequencing is done,
at least here at <a href="https://www.scilifelab.se/" target="_blank" rel="noopener">scilifelab</a>
 or
<a href="https://pharmb.io/" target="_blank" rel="noopener">pharmb.io</a>
 for example. This definition leaves out
a lot of details though, so we will need to dig a little bit deeper
below.</p>
<p>Secondly we also have to distinguish between the <em>workflow definition</em>,
and <em>concrete workflow invocations</em> based on this definition. We will in
this post mostly talk about the former one.</p>
<h2 id="the-smallest-constituent-of-a-workflow">The smallest constituent of a workflow</h2>
<p>One good way to define something, is generally by identifying its
smallest common denominator constituent, so let&rsquo;s do that.</p>
<p>Here again I think lies some confusion. When people think of the
smallest common constituent in a workflow, it is common to hear
suggestions such as the workflow &ldquo;steps&rdquo;, &ldquo;processes&rdquo;,
&ldquo;components&rdquo;, &ldquo;tasks&rdquo; and similar. I argue that not all of these
concepts are actually referring to the smallest common part of
workflows.</p>
<p>I argue that the smallest common constituent of a scientific batch
workflow definition is always <em><strong>unique program invocation
definitions</strong></em>. That is, a definition specifying a specific program
(with the version fixed) coupled with specific input data (e.g. specific
files with specific content, and specific parameter values). Note that
if anything changes in these constituting parts, even just a single bit
of an input file, this will result in creating a <strong>new</strong> unique program
invocation definition.</p>
<p>Unique program invocation definitions are mostly not what is referred to
when talking about workflow <em><strong>steps</strong></em>, <em><strong>components</strong></em> or
<em><strong>processes</strong></em>. It is, on the other hand, mostly what is meant with the
term <em><strong>workflow task</strong></em>. Thus, that is the term we&rsquo;ll use throughout
this post.</p>
<h2 id="building-on-the-smallest-constituting-part">Building on the smallest constituting part</h2>
<p>Now that we have defined the smallest constituent of a workflow
definition, it is much easier to try to identify various larger
constructs that builds upon this concept.</p>
<p>It turns out that terms such as <strong>workflow step</strong>, <strong>workflow process</strong>
and <strong>workflow component</strong>, typically are all variations of the same
thing, namely <em><strong>templates for generating unique program invocation
definitions.</strong></em> In other words, <em><strong>templates for generating tasks</strong>.</em></p>
<p>It turns out that such vastly different approaches to workflow design as
<a href="https://www.gnu.org/software/make/" target="_blank" rel="noopener">GNU Make</a>
 and its successors such
as <a href="http://snakemake.readthedocs.io" target="_blank" rel="noopener">Snakemake</a>
, dataflow-centric tools
like <a href="http://nextflow.io" target="_blank" rel="noopener">Nextflow</a>
 or <a href="http://scipipe.org/" target="_blank" rel="noopener">SciPipe</a>
,
other push-based but non-dataflow-based tools and paradigms like
<a href="http://www.cuneiform-lang.org/" target="_blank" rel="noopener">Galaxy</a>
,
<a href="http://pachyderm.io/" target="_blank" rel="noopener">Pachyderm</a>
 or the <a href="http://www.commonwl.org/" target="_blank" rel="noopener">Common Workflow Language
(CWL)</a>
, or tools wrapping commandline
execution in a functional language like
<a href="http://www.cuneiform-lang.org/" target="_blank" rel="noopener">Cuneiform</a>
, are all &ldquo;just&rdquo; variations
on the same thing: A templating mechanism for concrete program
invocation definitions, augmented with mechanisms for populating those
templates.</p>
<p>And this last part - the mechanism for populating the templates - is
where the inherent differences between all the mentioned tool approaches
and concrete tools lie. In make-like tools, workflow steps use
interpolation of file names to figure out concrete parameter values and
input file names, to populate the task template. In dataflow-centric and
other push-based systems, the templates are instead augmented with
dataflow connections pointing to the source from which concrete values
will be received once produced. In functional programming environments,
the inputs to the templates are instead generated from the function call
graph generated when calling a nested chain of functions depending on
each other.</p>
<h2 id="one-more-ingredient-dependencies-in-terms-of-data">One more ingredient: Dependencies (in terms of data)</h2>
<p>We have seen that most workflow tools are basically template-filling
machinery, built upon different paradigms. In the template-filling
machinery, we can identify one more common and re-occurring ingredient
though: dependencies. All tools provide some way of specifying how tasks
are dependent on other tasks.</p>
<p>In make-like systems, this is again done through file name patterns:
Dependencies are specified via the input file name patterns, that are
made concrete by interpolating file name patterns. In dataflow systems,
channels between components provide a kind of &ldquo;template&rdquo; for
dependencies: It provides a static connection which will serve as the
mechanism to generate any concrete dependencies on upstream files for
specific tasks. The other push-based systems work somewhat similarly to
dataflow systems, except with a more restricted model of how the
concrete dependencies are generated, as they typically lack <a href="http://bionics.it/posts/dynamic-workflow-scheduling" target="_blank" rel="noopener">dynamic
scheduling</a>
 (one
component does not typically produce more than one output per set of
inputs, for example, as could be the case for a dataflow-component).
Finally, in functional languages, the function call graph itself is the
dependency definition.</p>
<p>One caveat to look out for here: Some tools tend to define dependencies
only in terms of the finishing of tasks, not on the availability of the
outputs of that task, thus leaving out data flow specification from the
workflow definition. This is often problematic as it requires task
definitions to include internals of upstream tasks, in order to navigate
to the right output files. <a href="http://bionics.it/posts/dynamic-workflow-scheduling" target="_blank" rel="noopener">I have blogged about it
here</a>
, but in
summary, it is generally recommendable to specify the dependencies in
terms of data, not just task finish status.</p>
<h2 id="and-the-sum-of-all-this-is">And the sum of all this is?</h2>
<p>Ok, so let&rsquo;s conclude with a summary of this little foray into what
defines a scientific batch workflow:</p>
<p>A scientific batch workflow is:</p>
<ul>
<li>A set of concrete tasks (program invocation definitions)
<ul>
<li>With a specific program and version</li>
<li>With specific input data (such as files and parameter values)</li>
</ul>
</li>
<li>A set of dependencies between the inputs and outputs of the
aforementioned tasks</li>
</ul>
<p><strong>That&rsquo;s it</strong>, as far as I can see and understand. With this definition
in mind, it is - as said - much more interesting to go out and study how
different workflow tools are representing this information, and how they
make it easier for the user to generate the concrete set of tasks (which
can often be a very large number), from some more compact definition.</p>
<h2 id="so-what-about-the-dags">So what about the DAGs?</h2>
<p>I think the question of how to best represent workflows as DAGs is a
bigger topic that needs a separate post. But at least we can make some
notes based on the clarifications made here:</p>
<ul>
<li>There are multiple ways of representing workflows as DAGs
<ul>
<li>Nodes can mean anything between tasks, concrete task invocations
(concrete events in time), or some kind of task template</li>
<li>Edges can mean either data, or just dependency on task finish in
general</li>
</ul>
</li>
<li>The DAGs representing task templates (processes, steps or the like)
will be very different from DAGs representing tasks, or even worse,
concrete task invocations.
<ul>
<li>The DAGs representing tasks or task invocations are less likely
to be very intelligible, as they can easily contain thousands of
nodes.</li>
</ul>
</li>
</ul>
<p>So, if coming back to Jörgen&rsquo;s
<a href="https://twitter.com/joergenbr/status/907626987333746688" target="_blank" rel="noopener">tweet</a>
, I
think it is fair to say that &ldquo;a workflow is [just] a DAG&rdquo; is a bit
of a over-simplification. An over-simplification in more than one way,
it seems :)</p>
<hr>
<ul>
<li><strong>Update (Dec 7, 2017, 14:52 CET):</strong> Clarifications in the introduction</li>
<li><strong>Update (Dec 7, 2017, 15:07 CET):</strong> Fixed tons of typos, sorry</li>
<li><strong>Update (May 31, 2018, 20:53 CET):</strong> Vertices → Edges (Thanks Olivier
Mathieu!)</li>
</ul>

  
  <div>
    <div>Tags:</div>
    <ul>
        <li><a href="/tags/workflow-tools/">Workflow-Tools</a></li>
        <li><a href="/tags/scipipe/">Scipipe</a></li>
    </ul>
  </div>


  </main>
  <footer>
    <p>Copyright &copy; 2024 Living Systems. All rights reserved.</p>

  </footer>
</body>
</html>
